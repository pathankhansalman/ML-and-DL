{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1084f7c6-0063-4329-8653-1bd5ccd71996",
   "metadata": {},
   "source": [
    "## Deep ensembles based training implementation\n",
    "\n",
    "Implementing the methodology outlined in https://arxiv.org/pdf/1612.01474, except for the adversarial training. To modify to adversarial training, add gaussian noise to the training samples. Credit to the work goes completely to the authors and other parties mentioned in the paper, as per the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b83bc-2a3d-4b97-bf4e-3f176b4d1e8e",
   "metadata": {},
   "source": [
    "### Defining model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a634228e-cd14-430f-8b33-edee2c8849a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def _deep_ens_loss_(x, y):\n",
    "    return torch.mean(torch.log(x[:, 1]**2)/2 + (y - x[:, 0])**2/(2*x[:, 1]**2))\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply ReLU activation to hidden layer\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        # Apply output layer (no activation here, often softmax is applied externally if needed)\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c41a8b-6939-4fd1-a59d-d1cf42ca9be4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "10d626c9-b91f-4cdd-a4fb-0dcdba076395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 14850.8616\n",
      "Epoch [2/20], Loss: 14832.2363\n",
      "Epoch [3/20], Loss: 14832.9243\n",
      "Epoch [4/20], Loss: 14832.1628\n",
      "Epoch [5/20], Loss: 14835.2836\n",
      "Epoch [6/20], Loss: 14834.4737\n",
      "Epoch [7/20], Loss: 14834.2676\n",
      "Epoch [8/20], Loss: 14837.1324\n",
      "Epoch [9/20], Loss: 14887.1743\n",
      "Epoch [10/20], Loss: 14833.3569\n",
      "Epoch [11/20], Loss: 14832.8624\n",
      "Epoch [12/20], Loss: 14833.0183\n",
      "Epoch [13/20], Loss: 14833.2853\n",
      "Epoch [14/20], Loss: 14899.0095\n",
      "Epoch [15/20], Loss: 16218.0776\n",
      "Epoch [16/20], Loss: 14835.7335\n",
      "Epoch [17/20], Loss: 14833.5402\n",
      "Epoch [18/20], Loss: 15104.6339\n",
      "Epoch [19/20], Loss: 14833.6117\n",
      "Epoch [20/20], Loss: 14833.3544\n",
      "Epoch [1/20], Loss: 8492463.3216\n",
      "Epoch [2/20], Loss: 8492108.1181\n",
      "Epoch [3/20], Loss: 8492105.9874\n",
      "Epoch [4/20], Loss: 8492051.4348\n",
      "Epoch [5/20], Loss: 8492058.3910\n",
      "Epoch [6/20], Loss: 8492074.2371\n",
      "Epoch [7/20], Loss: 8492068.8603\n",
      "Epoch [8/20], Loss: 8492051.3046\n",
      "Epoch [9/20], Loss: 8494833.5934\n",
      "Epoch [10/20], Loss: 8492055.7010\n",
      "Epoch [11/20], Loss: 8492065.3097\n",
      "Epoch [12/20], Loss: 8492051.7324\n",
      "Epoch [13/20], Loss: 8492057.5615\n",
      "Epoch [14/20], Loss: 8492051.4511\n",
      "Epoch [15/20], Loss: 8492053.4749\n",
      "Epoch [16/20], Loss: 8492057.5071\n",
      "Epoch [17/20], Loss: 8494763.7268\n",
      "Epoch [18/20], Loss: 8492123.4568\n",
      "Epoch [19/20], Loss: 8651907.4708\n",
      "Epoch [20/20], Loss: 8492594.0088\n",
      "Epoch [1/20], Loss: 30045.1908\n",
      "Epoch [2/20], Loss: 30045.8852\n",
      "Epoch [3/20], Loss: 30045.3754\n",
      "Epoch [4/20], Loss: 30045.2204\n",
      "Epoch [5/20], Loss: 30045.2895\n",
      "Epoch [6/20], Loss: 30045.2026\n",
      "Epoch [7/20], Loss: 30045.1975\n",
      "Epoch [8/20], Loss: 30045.2505\n",
      "Epoch [9/20], Loss: 30045.3443\n",
      "Epoch [10/20], Loss: 30045.2625\n",
      "Epoch [11/20], Loss: 30045.2762\n",
      "Epoch [12/20], Loss: 30045.2361\n",
      "Epoch [13/20], Loss: 30045.2870\n",
      "Epoch [14/20], Loss: 30045.2385\n",
      "Epoch [15/20], Loss: 30045.5733\n",
      "Epoch [16/20], Loss: 30045.2785\n",
      "Epoch [17/20], Loss: 30045.2882\n",
      "Epoch [18/20], Loss: 30045.4815\n",
      "Epoch [19/20], Loss: 30045.2404\n",
      "Epoch [20/20], Loss: 30045.3280\n",
      "Epoch [1/20], Loss: 48002.5052\n",
      "Epoch [2/20], Loss: 48001.7685\n",
      "Epoch [3/20], Loss: 48001.7443\n",
      "Epoch [4/20], Loss: 48001.7441\n",
      "Epoch [5/20], Loss: 48002.4501\n",
      "Epoch [6/20], Loss: 48001.7619\n",
      "Epoch [7/20], Loss: 382644.0332\n",
      "Epoch [8/20], Loss: 48001.7631\n",
      "Epoch [9/20], Loss: 48001.7595\n",
      "Epoch [10/20], Loss: 48001.7868\n",
      "Epoch [11/20], Loss: 48001.7749\n",
      "Epoch [12/20], Loss: 48001.9472\n",
      "Epoch [13/20], Loss: 48001.7771\n",
      "Epoch [14/20], Loss: 48001.7564\n",
      "Epoch [15/20], Loss: 48001.9448\n",
      "Epoch [16/20], Loss: 48003.2545\n",
      "Epoch [17/20], Loss: 48001.7531\n",
      "Epoch [18/20], Loss: 48001.8129\n",
      "Epoch [19/20], Loss: 48001.7923\n",
      "Epoch [20/20], Loss: 48001.7540\n",
      "Epoch [1/20], Loss: 37277.2128\n",
      "Epoch [2/20], Loss: 37289.0241\n",
      "Epoch [3/20], Loss: 37276.4766\n",
      "Epoch [4/20], Loss: 37288.5894\n",
      "Epoch [5/20], Loss: 37276.5336\n",
      "Epoch [6/20], Loss: 37276.4112\n",
      "Epoch [7/20], Loss: 37276.4376\n",
      "Epoch [8/20], Loss: 37277.2212\n",
      "Epoch [9/20], Loss: 37276.3454\n",
      "Epoch [10/20], Loss: 37278.2068\n",
      "Epoch [11/20], Loss: 38774.2281\n",
      "Epoch [12/20], Loss: 37276.4930\n",
      "Epoch [13/20], Loss: 37285.5006\n",
      "Epoch [14/20], Loss: 37357.6968\n",
      "Epoch [15/20], Loss: 37452.3152\n",
      "Epoch [16/20], Loss: 37301.6354\n",
      "Epoch [17/20], Loss: 37276.3363\n",
      "Epoch [18/20], Loss: 37277.3397\n",
      "Epoch [19/20], Loss: 37276.7962\n",
      "Epoch [20/20], Loss: 37277.0113\n",
      "Epoch [1/20], Loss: 32116.8009\n",
      "Epoch [2/20], Loss: 32120.5405\n",
      "Epoch [3/20], Loss: 32113.9088\n",
      "Epoch [4/20], Loss: 32113.5842\n",
      "Epoch [5/20], Loss: 32114.0304\n",
      "Epoch [6/20], Loss: 32113.7338\n",
      "Epoch [7/20], Loss: 32113.6820\n",
      "Epoch [8/20], Loss: 32113.6379\n",
      "Epoch [9/20], Loss: 32115.0260\n",
      "Epoch [10/20], Loss: 32113.5925\n",
      "Epoch [11/20], Loss: 32113.7924\n",
      "Epoch [12/20], Loss: 32113.6428\n",
      "Epoch [13/20], Loss: 32113.6299\n",
      "Epoch [14/20], Loss: 32212.5207\n",
      "Epoch [15/20], Loss: 32114.2768\n",
      "Epoch [16/20], Loss: 32113.5890\n",
      "Epoch [17/20], Loss: 32116.1085\n",
      "Epoch [18/20], Loss: 32116.3377\n",
      "Epoch [19/20], Loss: 32113.9510\n",
      "Epoch [20/20], Loss: 32113.8026\n",
      "Epoch [1/20], Loss: 45135.0053\n",
      "Epoch [2/20], Loss: 45292.2900\n",
      "Epoch [3/20], Loss: 45135.2223\n",
      "Epoch [4/20], Loss: 45173.0208\n",
      "Epoch [5/20], Loss: 45257.6678\n",
      "Epoch [6/20], Loss: 45135.6476\n",
      "Epoch [7/20], Loss: 45415.1582\n",
      "Epoch [8/20], Loss: 45137.3666\n",
      "Epoch [9/20], Loss: 45136.6257\n",
      "Epoch [10/20], Loss: 45135.3915\n",
      "Epoch [11/20], Loss: 45138.4202\n",
      "Epoch [12/20], Loss: 45135.2597\n",
      "Epoch [13/20], Loss: 45626.7279\n",
      "Epoch [14/20], Loss: 45641.0403\n",
      "Epoch [15/20], Loss: 45439.4506\n",
      "Epoch [16/20], Loss: 45136.1689\n",
      "Epoch [17/20], Loss: 45136.4819\n",
      "Epoch [18/20], Loss: 45139.7793\n",
      "Epoch [19/20], Loss: 45140.0414\n",
      "Epoch [20/20], Loss: 45138.8034\n",
      "Epoch [1/20], Loss: 431963.0019\n",
      "Epoch [2/20], Loss: 431962.6982\n",
      "Epoch [3/20], Loss: 431962.6239\n",
      "Epoch [4/20], Loss: 431969.9201\n",
      "Epoch [5/20], Loss: 431975.7239\n",
      "Epoch [6/20], Loss: 431960.8909\n",
      "Epoch [7/20], Loss: 431965.9311\n",
      "Epoch [8/20], Loss: 432708.8118\n",
      "Epoch [9/20], Loss: 431961.4937\n",
      "Epoch [10/20], Loss: 431976.4710\n",
      "Epoch [11/20], Loss: 431962.4281\n",
      "Epoch [12/20], Loss: 431985.5741\n",
      "Epoch [13/20], Loss: 431961.9058\n",
      "Epoch [14/20], Loss: 431963.3834\n",
      "Epoch [15/20], Loss: 431962.7219\n",
      "Epoch [16/20], Loss: 431989.2974\n",
      "Epoch [17/20], Loss: 431964.6432\n",
      "Epoch [18/20], Loss: 431963.7972\n",
      "Epoch [19/20], Loss: 431967.1935\n",
      "Epoch [20/20], Loss: 432214.1036\n",
      "Epoch [1/20], Loss: 56547.7648\n",
      "Epoch [2/20], Loss: 56533.0412\n",
      "Epoch [3/20], Loss: 56612.3729\n",
      "Epoch [4/20], Loss: 56533.0685\n",
      "Epoch [5/20], Loss: 56534.1020\n",
      "Epoch [6/20], Loss: 56532.7476\n",
      "Epoch [7/20], Loss: 56539.9398\n",
      "Epoch [8/20], Loss: 56585.4737\n",
      "Epoch [9/20], Loss: 56643.4977\n",
      "Epoch [10/20], Loss: 56534.2488\n",
      "Epoch [11/20], Loss: 56533.6655\n",
      "Epoch [12/20], Loss: 56543.1611\n",
      "Epoch [13/20], Loss: 56533.9991\n",
      "Epoch [14/20], Loss: 56550.2847\n",
      "Epoch [15/20], Loss: 56532.9062\n",
      "Epoch [16/20], Loss: 56533.2223\n",
      "Epoch [17/20], Loss: 56534.2813\n",
      "Epoch [18/20], Loss: 56534.4303\n",
      "Epoch [19/20], Loss: 56533.3941\n",
      "Epoch [20/20], Loss: 56533.1322\n",
      "Epoch [1/20], Loss: 3064700.5556\n",
      "Epoch [2/20], Loss: 3064750.2939\n",
      "Epoch [3/20], Loss: 3064700.7893\n",
      "Epoch [4/20], Loss: 3064700.6253\n",
      "Epoch [5/20], Loss: 3064700.6941\n",
      "Epoch [6/20], Loss: 3064700.5688\n",
      "Epoch [7/20], Loss: 3064795.8605\n",
      "Epoch [8/20], Loss: 3064700.4346\n",
      "Epoch [9/20], Loss: 3064703.4786\n",
      "Epoch [10/20], Loss: 3064703.9203\n",
      "Epoch [11/20], Loss: 3064700.9401\n",
      "Epoch [12/20], Loss: 3064700.4123\n",
      "Epoch [13/20], Loss: 3064700.8621\n",
      "Epoch [14/20], Loss: 3064700.7434\n",
      "Epoch [15/20], Loss: 3064700.4982\n",
      "Epoch [16/20], Loss: 3064748.6546\n",
      "Epoch [17/20], Loss: 3064700.3694\n",
      "Epoch [18/20], Loss: 3064700.7492\n",
      "Epoch [19/20], Loss: 3064700.8253\n",
      "Epoch [20/20], Loss: 3064700.6384\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 10     # Number of input features\n",
    "hidden_size = 5     # Number of neurons in the hidden layer\n",
    "output_size = 2     # Number of output neurons (for a binary classification task)\n",
    "num_epochs = 20     # Number of training epochs\n",
    "batch_size = 128     # Batch size for training\n",
    "learning_rate = 0.0001  # Learning rate for the optimizer\n",
    "num_ens = 10  # Number of ensembles\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = _deep_ens_loss_\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Use SGD optimizer\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "num_samples = 10000\n",
    "X = torch.randn(num_samples, input_size)  # Randomly generated input features\n",
    "y = torch.randn(num_samples,)  # Randomly generated binary labels (0 or 1)\n",
    "dataset = TensorDataset(X, y)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "models = []\n",
    "for i in range(num_ens):\n",
    "    # Instantiate the model\n",
    "    model = SimpleNN(input_size, hidden_size, output_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in data_loader:\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            running_loss += loss.item()\n",
    "        avg_loss = running_loss / len(data_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf07b8-c5da-4db5-8ad3-800c8b43d4b6",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9045c714-cef1-4087-b377-1d8987121fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset and DataLoader\n",
    "test_num_samples = 3000\n",
    "X_test = torch.randn(test_num_samples, input_size)  # Randomly generated input features\n",
    "y_test = torch.randn(test_num_samples,)   # Randomly generated binary labels (0 or 1)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "for test_inputs, _ in test_data_loader:\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        test_outputs = [model(test_inputs) for model in models]\n",
    "test_means = reduce(lambda x, y: torch.add(x, y), [output[:, 0] for output in test_outputs])/num_ens\n",
    "test_var = reduce(lambda x, y: torch.add(x, y),\n",
    "                  [torch.add(torch.pow(output[:, 0], 2), torch.pow(output[:, 1], 2))\n",
    "                   for output in test_outputs])/num_ens - torch.pow(test_means, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725039d0-28c3-45c5-b8cc-7b6bcb66630d",
   "metadata": {},
   "source": [
    "test_means has the mean and test_var has the variance for gaussian distribution of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "436bbb0d-2151-4c9d-89d4-03b65bc6ea96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1013, -0.1611, -0.1012,  ..., -0.1059, -0.1488, -0.1170])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "db78e833-5901-4fc1-ae49-d8657159279c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2633, 0.2263, 0.1702,  ..., 0.1490, 0.1994, 0.1258])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe151e50-3579-4355-9ebc-76628910004e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
